import requests
from bs4 import BeautifulSoup
import base64
import os
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
}

# ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
base_dir = os.path.dirname(os.path.abspath(__file__))
save_dir = os.path.join(base_dir, '../img/soccerTeam/laligaSpain')
os.makedirs(save_dir, exist_ok=True)
print("ğŸ“ ì €ì¥ ë””ë ‰í† ë¦¬:", save_dir)

# í¬ë¡¤ë§í•  URL ëª©ë¡
urls = [
    "https://namu.wiki/w/2025-26%20%EB%9D%BC%EB%A6%AC%EA%B0%80%20EA%20SPORTS",
    "https://namu.wiki/w/2024-25%20%EB%9D%BC%EB%A6%AC%EA%B0%80%20EA%20SPORTS",
    "https://namu.wiki/w/2023-24%20%EB%9D%BC%EB%A6%AC%EA%B0%80%20EA%20SPORTS",
    "https://namu.wiki/w/2021-22%20%EB%9D%BC%EB%A6%AC%EA%B0%80%20EA%20SPORTS",
    "https://namu.wiki/w/2020-21%20%EB%9D%BC%EB%A6%AC%EA%B0%80%20EA%20SPORTS",
    "https://namu.wiki/w/2019-20%20%EB%9D%BC%EB%A6%AC%EA%B0%80%20EA%20SPORTS",

]

club_data = set()

for url in urls:
    try:
        print(f"ğŸ” ìš”ì²­ ì¤‘: {url}")
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        span_tags = soup.find_all('span', class_='no5JPXQH')
        for span in span_tags:
            img = span.find('img', class_='IIlhLQ+O')
            if img:
                alt = img.get('alt')
                src = img.get('data-src') or img.get('src')  # âœ… ì¤‘ìš” í¬ì¸íŠ¸!

                if alt and "êµ­ê¸°" in alt:
                    continue

                if src and src.startswith('//'):
                    src = 'https:' + src

                if alt and src:
                    club_data.add((alt.strip(), src.strip()))
                keywords = ['FC', 'CF', 'í´ëŸ½', 'êµ¬ë‹¨', 'ë¡œê³ ', 'ì— ë¸”ëŸ¼']

                if alt and not any(keyword in alt for keyword in keywords):
                    continue


    except Exception as e:
        print(f"[X] í¬ë¡¤ë§ ì‹¤íŒ¨: {url} â†’ {e}")

for alt, src in sorted(club_data):
    try:
        safe_name = re.sub(r'[\\/*?:"<>|]', "", alt)

        # Base64 ì´ë¯¸ì§€
        if src.startswith('data:image'):
            header, b64_data = src.split(',', 1)
            ext = '.svg' if 'svg+xml' in header else '.png'
            file_path = os.path.join(save_dir, f"{safe_name}{ext}")

            with open(file_path, 'wb') as f:
                f.write(base64.b64decode(b64_data))
            print(f"[â†“] ì €ì¥ ì™„ë£Œ (Base64): {alt} â†’ {file_path}")
            continue

        # ì¼ë°˜ URL ì´ë¯¸ì§€
        if src.startswith('//'):
            src = 'https:' + src

        ext = os.path.splitext(src)[-1]
        if not ext or len(ext) > 5 or '?' in ext:
            ext = '.svg' if '.svg' in src else '.png'
        file_path = os.path.join(save_dir, f"{safe_name}{ext}")

        img_res = requests.get(src, headers=headers)
        img_res.raise_for_status()

        content = img_res.content

        # SVG ë‚´ìš© ë¶„ì„ (ë¹ˆ íŒŒì¼ ë˜ëŠ” <use>ë§Œ ìˆëŠ” ê²½ìš° ê²½ê³ )
        if ext == '.svg' and b'<svg' in content:
            text = content.decode("utf-8", errors="ignore")
            if '<use' in text and 'xlink:href' in text:
                print(f"âš ï¸ SVGëŠ” ì™¸ë¶€ <use> ì°¸ì¡°ë§Œ í¬í•¨ (ë Œë”ë§ ë¶ˆê°€ ê°€ëŠ¥ì„±): {alt}")
            elif len(text.strip()) < 50:
                print(f"âš ï¸ SVG ë‚´ìš©ì´ ê±°ì˜ ì—†ìŒ: {alt}")

        with open(file_path, 'wb') as f:
            f.write(content)

        print(f"[â†“] ì €ì¥ ì™„ë£Œ: {alt} â†’ {file_path}")

    except Exception as e:
        print(f"[X] ì €ì¥ ì‹¤íŒ¨: {alt} â†’ {e}")
